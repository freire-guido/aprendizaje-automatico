# Trabajo Pr치ctico - Aprendizaje supervisado
## Clasificaci칩n de expresiones gen칩micas

### 츼rbol de indecisi칩n
### Integrantes:
- Freire, Guido LU: 978/21
- Motta, Facundo LU: 889/21
- Rodriguez, Ignacio LU: 956/21
- Wisznia, Juan LU: 520/16

---

# Ejercicio 1
## Separaci칩n de datos

Dado que tenemos una base de datos acotada (500 datos), consideramos que utilizar el 10% de los datos como test set, es poco confiable (50 datos). Consideramos que 50 datos no nos proporcionan realmente una certeza acorde a la que buscamos para testear que tan bueno es nuestro modelo. Con este objetivo, decidimos separar el 20% de los datos para test, porque queremos una buena estimaci칩n del AUCROC para el modelo final. Para esto implementamos la funci칩n ```desarrollo_evaluacion```, que toma las 칰ltimas filas del dataframe. 

Sin embargo, los datos podr칤an estar "cargados" al dataset de cierta manera provocando asi que los ultimos datos del dataset tengan un patron que el modelo sea incapaz de generalizar. En este sentido buscamos que el conjunto de testeo sea una buena representaci칩n del dataset, entonces antes de separar los datos permutamos las filas del dataframe de forma aleatoria usando la funci칩n ```sample``` de pandas (toma una muestra de todo el dataset, sin reposicion. Es decir, lo permuta).

Una vez separados los datos en un conjunto de entrenamiento y un conjunto de evaluacion o test, aplicaremos preferentemente K-fold cross validation (a menos que una consigna indique otra forma de entrenamiento) y todos los modelos o metodos sobre el conjunto de entrenamiento sin tocar el conjunto de test, para no obtener ni inferir informacion respecto de este conjunto. Finalmente, al momento de indicar el indice de certeza que tiene nuestro modelo utilizaremos este conjunto de evaluacion, sobre el cual aplicaremos el modelo que consideremos pertinente y del cual, en ningun momento habremos obtenido informacion al respecto, como si predicieramos datos "nuevos" en nuestro data frame.

# Ejercicio 2
## Construcci칩n de modelos

1. Entrenamos el 치rbol de decisi칩n de altura m치xima 3 y estimamos con K-fold cross validation usando ```model_selection.cross_validate``` de sklearn con
cv = 5.

2. Para calcular el AUPRC usamos ```average_precision```, que calcula el 치rea debajo de la curva PRC **sin interpolar**. Osea, algo m치s bien parecido
a la suma de Riemann:

$$ \text{AP} = \sum_n (R_n - R_{n-1})P_n $$

donde $R_n$ es el recall y $P_n$ la precisi칩n. Los resultados que obtuvimos estan en la tabla:

| Permutaci칩n | Accuracy (training) | Accuracy (validaci칩n) | AUPRC (training) | AUPRC (validaci칩n) | AUCROC (training) | AUCROC (validaci칩n) |
| -----       | -----               | -----                 | -----            | -----              | -----             | -----               |
| 1           | 0.821               | 0.687                 | 0.740            | 0.475              | 0.840             | 0.650               |
| 2           | 0.759               | 0.662                 | 0.625            | 0.425              | 0.800             | 0.666               |
| 3           | 0.853               | 0.737                 | 0.737            | 0.507              | 0.835             | 0.737               |
| 4           | 0.840               | 0.650                 | 0.718            | 0.343              | 0.839             | 0.550               |
| 5           | 0.828               | 0.700                 | 0.723            | 0.480              | 0.860             | 0.666               |
| Promedios   | 0.820               | 0.687                 | 0.709            | 0.446              | 0.837             | 0.654               |
| Global      | (NO)                | 0.687                 | (NO)             | 0.397              | (NO)              | 0.619               |

*Tabla 2.1*

3. Definimos una ```ParameterGrid``` de sklearn con los par치metros que queremos probar con el 치rbol de decisi칩n y usamos ```cross_validate``` con la m칠trica accuracy. Los resultados fueron:

| Altura m치xima | Criterio de corte | Accuracy (training) | Accuracy (validaci칩n) |
| -----         | -----             | -----               | -----                 |
| 3             | Gini              | 0.820               | 0.687                 |
| 5             | Gini              | 0.938               | 0.657                 |
| Inf           | Gini              | 1.000               | 0.650                 |
| 3             | Entrop칤a          | 0.795               | 0.717                 |
| 5             | Entrop칤a          | 0.908               | 0.657                 |
| Inf           | Entrop칤a          | 1.000               | 0.655                 |

*Tabla 2.2*

4. De la tabla 2.1 vemos que los folds obtienen scores parecidos dentro de una m칠trica. Interpretamos que los datos est치n representados de forma pareja en cada uno de ellos. Tiene sentido que el promedio sea cercano a cada uno de los folds. Tambi칠n cabe notar que usar la evaluaci칩n *Global* recomendada en clase es muy cercana al promedio, pero siempre por debajo del mismo. Seg칰n 2.1 la peor m칠trica para este modelo es AUPRC.

En la tabla 2.2 es claro que subir la altura m치xima del 치rbol aumenta la precisi칩n en training. De hecho, al entrenar un 치rbol completo cada dato de entrenamiento est치 bien clasificado (precisi칩n 1). Sin embargo la varianza es muy alta ("overfitting") y la precisi칩n empeora. El criterio de corte entrop칤a es marginalmente mejor que gini.

# Ejercicio 3
## Comparacion de algoritmos

Comparemos la certeza de ciertos algoritmos de clasificacion mediante la metrica AUCROC resultante de un 5-fold cross validation. En este caso compararemos los algoritmos:
* 츼rboles de decisi칩n (ADD)
* KNN (k-vecinos m치s cercanos)
* SVM (Support vector machine)
* LDA (Linear discriminant analysis)
* Na칦ve Bayes

Adem치s compararemos "intra" algoritmos, es decir, compararemos mismos algoritmos con distintos hiperparametros y asi poder analizar como afectan el rendimiento de cada algoritmo. En este sentido, utilizaremos un ```RandomizedSearchCV``` que nos permite buscar de forma aleatoria en la grilla o "hipercubo" que generan los distintos intervalos/regiones para cada hiperparametro de cada modelo.

Comenzaremos delimitando cuales hiperparametros utilizar para cada modelo y sus respectivos intervalos (*Tabla 3.1*) y/o valores que pueden tomar los mismos. Los dominios de cada hiperpar치metro los elegimos "a ojo", leyendo la documentaci칩n de cada modelo y pensando valores razonables.

| Modelo        | Hiperpar치metro    | Intervalo / Valores posibles      | Descripci칩n           |
| -----         | -----             | -----                             | -----                 |
| ADD           | max_depth         | [1,2, ..., 19]                    | Altura maxima del arbol|
| ADD           | Criterion         | ['gini', 'entropy', 'log_loss']   | Estrategia utilizada para separar en cada nodo|
| ADD           | max_features      | ['sqrt', 'log2']                  | Numero de features al considerar el mejor corte|
| KNN           | n_neighbors       | [1,2, ..., 19]                    | Cantidad de vecinos mas cercanos a observar|
| KNN           | metric            | ['l1', 'l2', 'cosine']            | Metrica utilizada para calcular la distancia entre observaciones|
| SVM           | C                 | [1, 1.5, 2, ...., 19.5]          | Citerio de regularizacion, inversamente proporcional a la fuerza de regularizacion buscada|
| SVM           | tol               | [1e-1, 1e-2, ..., 1e-5]           | Criterio de tolerancia para frenar algoritmo|
| SVM           | kernel            | ['linear', 'rbf']                 | Nucleo utilizado en el algoritmo, "genera" la forma de las regiones delimitadas|
| LDA           | solver            | ['lsqr', 'eigen']                 | Metodo utilizado para resolver el problema|
| LDA           | shrinkage         | [None, 'auto', 0.1, 0.5, 1.0]     | Controla si se utiliza o no el enfoque de "contraccion" de la amtriz de covarianza|
| Naive Bayes   | priors            | [0, 0.01, 0.02, ...., 0.99]       | Probabilidades inciales para las clases|

*Tabla 3.1*

Para analizar la performance imprimimos los valores que se guardan en ```RandomizedSearchCV.cv_results_```. Los resultados completos est치n en el c칩digo, pero en la *Tabla 3.2* mostramos las mejores (游땎) y peores (游땬) combinaciones de hiperpar치metros para cada modelo:

### 츼rbol de decisi칩n
| max_features        | max_depth    | criterion      | score           |
| -----               | -----        | -----          | -----           |
| log2                | 1            | gini           | 0.594 游땎        |
| log2                | 10           | log_loss       | 0.579 游땎        |
| log2                | 8            | gini           | 0.526 游땬        |
| log2                | 1            | log_loss       | 0.505 游땬        |

Es llamativo que la mejor y peor combinaci칩n son iguales, excepto que la mejor usa gini y la peor log_loss. Algo parecido pasa con la segunda mejor y peor combinaci칩n, que tienen una altura profunda (10 y 8) pero esta vez el gini es el que tiene peor rendimiento!
Igualmente, el rendimiento del modelo es bastante pobre para todas las configuraciones.

### KNN
| n_neighbors         | metric       | score           |
| -----               | -----        | -----           |
| 14                  | l1           | 0.840 游땎        |
| 17                  | 11           | 0.838 游땎        |
| 19                  | l2           | 0.812 游땎        |
| 6                   | l2           | 0.778 游땬        |
| 1                   | l1           | 0.643 游땬        |

Ac치 es claro que el par치metro n_neghbors (que es el K de KNN) funciona mejor con valores entre 10-20 y funciona mal con valores <10.
El modelo tiene performance mejor que la del 치rbol de decisi칩n. Incluso para las peores configuraciones.

### SVM

| param_tol | param_kernel | param_C | mean_test_score |
| --------- | ------------ | --------| --------------- |
| 0.1       | rbf          | 9.526   | 0.891 游땎        |
| 0.001     | rbf          | 6.684   | 0.891 游땎        |
| 0.1       | linear       | 8.105   | 0.847 游땬        |
| 0.01      | linear       | 8.105   | 0.847 游땬        |

El kernel rbf tiene mejor performance. Es decir, los datos se pueden discriminar usando la distancia entre ellos. Esto tiene sentido si recordamos que KNN tambi칠n tuvo buenos resultados; otro m칠todo que explota la distancia entre instancias para predecir.

### LDA y Naive Bayes

| param_solver | param_shrinkage | mean_test_score |
| ------------ | --------------- | --------------- |
| lsqr         | 0.1             | 0.888 游땎        |
| eigen        | 0.1             | 0.888 游땎        |
| eigen        | 1.0             | 0.763 游땬        |

| param_priors                           | mean_test_score |
| -------------------------------------- | --------------- |
| [0.979, 0.020]                         | 0.822 游땎        |
| [0.151, 0.848]                         | 0.822 游땎        |
| [0.0, 1.0]                             | 0.500 游땬        |

Los par치metros para estos modelos no parecen tener un peso tan fuerte en las predicciones finales. Para LDA quiz치 no se explor칩 el espacio lo suficiente como para dar con combinaciones malas, pero en general el modelo tiene buen desempe침o.

Con Naive Bayes el modelo converge sin importar el prior por la cantidad de datos y el problema en s칤, que es de clasificaci칩n binaria.

# Ejercicio 4
## Diagn칩stico Sesgo-Varianza.

Con el objetivo de analizar el sesgo y la varianza de los mejores modelos obtenidos (mayor AUCROC) de los distintos algoritmos, utilizaremos modelos entrenados variando los hiperpar치metros o la cantidad de datos. Estas t칠cnicas nos permitir치n analizar y comprender cuando un modelo tiene o podr칤a tener alto/bajo sesgo y/o varianza. En este sentido, comenzaremos variando los hiperpar치metros ```max_depth``` y ```C``` de los modelos 치rbol de decisi칩n y SVM respectivamente, los AUCROC obtenidos se observan en el _Gr치fico 4.1_ a los que llamamos curvas de complejidad. 

El modelo de 치rbol de decisi칩n parece tener un sesgo alto cuando el valor del ```max_depth``` es bajo (no capta el patr칩n de los datos de entrenamiento). A medida que este aumenta, se observa que la m칠trica mejora significativamente en el caso del conjunto de entrenamiento, no as칤 en el conjunto de evaluaci칩n/test. Esto podr칤a deberse a varias razones, por ejemplo, nuestro 치rbol podr칤a estar sobreajustando, si esto ocurriera nuestro 치rbol no encontrar치 patrones significativamente generalizables dando as칤 una baja m칠trica en el conjunto de test. Sin embargo, esta idea quedar칤a f치cilmente descartada al observar que el conjunto de evaluaci칩n, sin importar el valor de ```max_depth```, siempre es bajo y en ning칰n momento logra un pico claro o la convergencia hacia alg칰n valor. Si comparamos el rendimiento del 치rbol de decisi칩n con el de los modelos SVM podemos observar un sobreajuste de el primero, se obtiene un mejor AUCROC sobre el conjunto de evaluaci칩n para valores m치s bajos en el conjunto de entrenamiento. 

Podr칤amos concluir entonces, que el causante de que nuestra m칠trica sea baja y por ende un modelo malo, es la alta varianza del mismo. El modelo puede ser muy sensible al ruido en los datos de entrenamiento. El 치rbol de decisi칩n no es un modelo lo suficientemente robusto para los datos con los que estamos trabajando. Veamos que sin importar el valor de la altura m치xima del 치rbol, el AUCROC del conjunto de test no logra superar en ning칰n momento un valor considerable como para tener en cuenta al modelo como "bueno". 

En el caso del SVM, ocurre exactamente lo opuesto. Se puede observar que cuando movemos el hiperpar치metro ```C```, no var칤a mucho el valor de la m칠trica tanto en el conjunto de entrenamiento como en el de test. Por ende, parecer칤a que el modelo es lo suficientemente robusto como para captar valores o condiciones m치s generales de los datos. La amplitud entre la curva de entrenamiento y la de test, se mantiene casi constante, el modelo se adapta muy bien a los patrones del conjunto de entrenamiento pero sigue aumentando su rendimiento en el conjunto de evaluaci칩n. Por 칰ltimo, podr칤amos decir que la casi nula variaci칩n (convergencia) en la m칠trica del conjunto de test indicar칤a una baja varianza del algoritmo, ya que para cualquier valor de ```C``` capta generalidades que logran una buena m칠trica. 

![curvas de complejidad](/tp/01_aprendizaje_supervisado/curvas_complejidad.png)
<p align="center"><em> Grafico 4.1</em> </p>

En la siguiente parte para continuar con el an치lisis Sesgo-Varianza veremos qu칠 ocurre con los modelos cuando variamos la cantidad de datos con las que entrenan, es decir, variamos la cantidad de instancias del conjunto de entrenamiento. No entraremos muy en detalle para cada modelo en particular, pero compararemos que diferencias o similitudes hay entre estos al momento de aplicar lo dicho previamente. En este sentido, se hace el gr치fico de las curvas de aprendizaje de los modelos (_Grafico 4.2_). 

Veamos entonces que en el caso del primer modelo (ADD) a medida que se aumenta el conjunto de entrenamiento, no se observa una mejora en la m칠trica del conjunto de test a diferencia de lo que pasa con el resto de modelos en donde, a medida que aumenta la cantidad de datos de entrenamiento este m칠trica si mejora, deduciendo as칤 que el ADD tiene un sesgo alto, esto significa que el modelo se ajusta demasiado a los datos de entrenamiento. A medida que aumenta la cantidad de datos, se observa una clara variaci칩n en su rendimiento en el conjunto de entrenamiento y no puede generalizar bien a nuevos datos. Esto se corresponde con una varianza alta. 

Al observar ahora los gr치ficos de los distintos modelos que no son el ADD, sugerir칤an que el rendimiento puede mejorar con el incremento de datos en el conjunto de entrenamiento dejando en claro as칤, el bajo sesgo de los mismos en comparaci칩n con el modelo de ADD. Adem치s, el valor de AUCROC para el cual estar칤an convergiendo pareciera lo suficientemente alto deduciendo de esta otra manera poca varianza en los mismos. 

Finalmente podemos observar el modelo GaussianNB en particular comparado con modelos como el SVM o el LDA. La convergencia del GaussianNB parecer칤a converger en un valor cercano al 0.7 a diferencia del SVM o LDA que ronda el 0.9. Esta diferencia podr칤a deducirse de un sobreajuste del modelo y por ende una varianza mayor del mismo. Decimos que este modelo sobreajusta ya que el modelo GaussianNB en el conjunto de entrenamiento obtiene una m칠trica cercana a 1, la cual casi no varia a medida que aumentan los datos, sin embargo, la convergencia del AUCROC en el conjunto de test es m치s modesta que en otros casos ya nombrados. 

![curvas de aprendizaje](/tp/01_aprendizaje_supervisado/curvas_aprendizaje.png)
<p align="center"><em> Grafico 4.2</em> </p>

Para finalizar con este an치lisis haremos un modelo de Random Forest, con 200 치rboles que luego mediante una votaci칩n "democr치tica" definir치n la clase correspondiente. Analizaremos la variable ```max_features``` que define el n칰mero m치ximo de variables aleatorias que se consideran al dividir un nodo durante la construcci칩n de cada 치rbol. 

En el _Grafico 4.3_ podemos ver que al variar el valor de ```max_features``` no se logra un mejor rendimiento del random forest. Nosotros entendemos que esto se debe a un balance entre el sesgo y la varianza del modelo, ya que un valor de ```max_features``` alto podr칤a sobre ajustar los 치rboles provocando as칤 una alta varianza, mientras que un valor bajo de ```max_features``` puede aumentar el sesgo al limitar la capacidad del modelo para capturar generalidades en los datos. 

![curvas de complejidad random forest](/tp/01_aprendizaje_supervisado/random_forest_complejidad.png)
<p align="center"><em> Grafico 4.3</em> </p>

Continuando con el an치lisis del RF (Random Forest), veamos que ocurre con este modelo cuando variamos la cantidad de datos con la que entrena, observemos entonces la curva de aprendizaje del modelo _Grafico 4.4_. Se logra observar una mejora en el rendimiento (incremento de la m칠trica AUCROC), sin embargo, se observa una convergencia de este modelo cercana al 0.8, menor que la de algunos modelos analizados previamente como el SVM o el LDA. 

De cualquier manera, este incremento en la m칠trica era predecible, ya que, generalmente, al acceder a m치s datos, el modelo puede encontrar generalidades que le servir치n para predecir y/o obtener una mejor m칠trica en un conjunto no visto hasta entonces, ya que se le ampl칤a el universo de datos vistos permitiendo observar datos que en iteraciones anteriores (las que ten칤an menos datos) no exist칤an. Podemos decir entonces, que es menos probable que el modelo sufra de un sesgo alto mientras m치s datos se le provean. 


![curvas de aprendizaje random forest](/tp/01_aprendizaje_supervisado/random_forest_aprendizaje.png)
<p align="center"><em> Grafico 4.4</em> </p>

# Ejercicio 5
## Evaluaci칩n de performance

Para cerrar el trabajo, lo que haremos ser치 predecir y ver que resultados no arrojan los datos separados en el conjunto de test. Adem치s tambi칠n trabajaremos con un conjunto de datos para los cuales no tenemos la etiqueta verdadera los mismos (```X_held_out```), nuestro objetivo ser치 ver que probabilidades nos asigna para cada instancia nuestro modelo de pertenecer a la clase 1, recordemos que est치bamos hablando de un problema binario, y intentar a su vez predecir el valor que el AUCROC tendr치 en ese mismo conjunto de datos.  

En este sentido es entonces que a partir del punto 3, observamos que el SVM tiene un mejor rendimiento que el resto de los modelos, por lo que lo seleccionamos para estimar las probabilidades. Para ver cuales son los mejores hiperpar치metros para este mismo, realizamos un Random Search de los hiperpar치metros ```tol``` y ```C```, mientras dejamos fijo el hiperpar치metro kernel como ```rbf```, esto es porque vimos que ```rbf``` lograba una mejor m칠trica en comparaci칩n a ```linear``` (la otra opci칩n que hab칤amos tenido en cuenta).  

![curvas de aprendizaje](/tp/01_aprendizaje_supervisado/AUCROC_SVM.png) 
<p align="center"><em> Grafico 5.1</em> </p> 

En el _Grafico 5.1_ se muestra una comparativa de los distintos valores de ```tol``` y ```C```, para los cuales se ejecut칩 un modelo en la funci칩n del Random Search. Podemos ver que el hiperpar치metro ```C``` no afecta tanto al rendimiento, es decir, si "clavamos" el valor de ```tol``` en alg칰n valor determinado, variar el ```C``` no produce mucha variaci칩n en el valor del AUCROC obtenido por el modelo. Diferente es con el hiperpar치metro ```tol```, para el cual si pareciera haber mejores valores que otros. Nos quedaremos entonces con los mejores valores de ```tol``` y ```C``` que generan el mayor valor de AUCROC entre todos los modelos. Estos valores son ($C=208602408924850.5$, $tol=0.08316104153230962$). 

Con este modelo entonces calcularemos la probabilidad de cada instancia del conjunto ```X_held_out``` de pertenecer a la clase 1. Pero adem치s intentaremos predecir al valor que la m칠trica AUCROC arrojar칤a sobre estos datos si efectivamente tuvi칠ramos las etiquetas reales de los mismos. Para esto lo que haremos ser치 utilizar el conjunto de test, veremos que etiqueta le asigna a cada instancia de estos datos y lo compararemos con la etiqueta real, que en este caso si tenemos. El AUCROC del conjunto de test nos arroja un valor del 0.9142, podr칤amos decir que este valor ser치 similar al del ```X_held_out``` predicho pues estos conjuntos ser칤an muestras de la misma distribuci칩n que los que tenemos para los conjuntos de entrenamiento y test. 

# Ejercicio 6
## Conclusiones

(COMPLETAR)

(COMENTARIO FACU)
les dejo esto por si les sirve para la conclusion, esta claramente sacado de chat gpt (MODIFIQUENLO o ni lo usen, lo q quieran), estaba buscando una conclusion general par ael punto 4, pero decidi dejarlo asi, asi todas la conlcusiones van aca al final

""""
Al comparar el sesgo y la varianza entre diferentes modelos, es crucial encontrar un equilibrio que permita desarrollar modelos con capacidad de generalizaci칩n 칩ptima. Un alto sesgo puede indicar una simplificaci칩n excesiva de los datos, mientras que una alta varianza sugiere sensibilidad excesiva a variaciones en los datos de entrenamiento. Encontrar el punto medio adecuado es esencial para asegurar un rendimiento estable y preciso en la predicci칩n de nuevos datos.
""""

(IDEAS NACHO PUNTO 4) (hay q chequear algunas cosas xd)

Decision tree tiene mucho sesgo con profundidad baja (No encuentra el patr칩n de los datos de train).
Decision tree, podemos hablar de overfitting si hay un m치ximo en el test, no hay uno claro. Pero si lo comparamos con el gr치fico del SVM se ve que hay una funci칩n con error en training similar pero mejor en test. Seg칰n la definici칩n de las diapos clase 1 podr칤a sobreajustar y subajustar pero ni idea. 

Para el gr치fico del SVM para un C m치s bajo el modelo subajusta (lo comparamos con un C m치s alto y se ve). Lo mismo para el Decision tree con poca profundidad vs cualquier C en el SVM.

En los gr치ficos de abajo:
Los primeros dos tienen mucha varianza, le agregamos datos y cambia mucho el rendimiento en train.
En decision tree no se ve que el rendimiento pueda mejorar con m치s datos (sesgo).

Los gr치ficos que no son del Decision Tree sugieren que el rendimiento pueda mejorar con m치s datos (poco sesgo) y parecen tener poca varianza al agregar datos.

GaussianNB si lo comparamos con otros sobreajusta



